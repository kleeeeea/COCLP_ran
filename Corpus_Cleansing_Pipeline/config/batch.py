# data_pipeline_refactor.py
"""
é€šç”¨å¹¶å‘å¤„ç†æ¡†æ¶ï¼ˆ**ä¸ä¸šåŠ¡è§£è€¦**ï¼‰
================================

> **æ ¸å¿ƒæ€æƒ³**ï¼šæŠŠ *çº¿ç¨‹æ± å¹¶å‘*ã€*æ¨¡å‹ç«¯ç‚¹è½®è¯¢*ã€*æ‰¹é‡æ–‡ä»¶éå†*ã€*æ–­ç‚¹ç»­è·‘* ç­‰**æ¨ªåˆ‡å…³æ³¨ç‚¹**å®Œå…¨æŠ½ç¦»å‡ºæ¥ï¼Œä¸šåŠ¡å±‚åªéœ€å®ç°ä¸€ä¸ª *record â†’ record* çš„çº¯å‡½æ•°å³å¯ã€‚
>
> è¿™æ ·æ— è®ºä½ è¦åš *åˆ†ç±»*ã€*çº é”™*ã€*æ‘˜è¦* è¿˜æ˜¯ä»»ä½•åˆ«çš„ä»»åŠ¡ï¼Œåªè¦æŠŠå¯¹åº”çš„å¤„ç†å‡½æ•°å¡è¿›æ¥ï¼Œå°±èƒ½å¤ç”¨å…¨éƒ¨â€œåŸºç¡€è®¾æ–½â€ã€‚

å˜æ›´æ¦‚è¦
--------
* åŸæ¥çš„ `ClassifyPipeline`ã€`classifier_fn` ç­‰**åˆ†ç±»ç‰¹å®šå‘½å**å·²æ”¹ä¸ºé€šç”¨ï¼š
  * `TaskPipeline` â€“ è´Ÿè´£å¹¶å‘é©±åŠ¨ä¸è½ç›˜ã€‚
  * `process_fn` â€“ ä½ çš„ä¸šåŠ¡å‡½æ•°ç­¾å `def fn(record, dispatcher) -> record:`ã€‚
  * `batch_process()` â€“ é’ˆå¯¹æ•´ä¸ªæ–‡ä»¶å¤¹çš„æ‰¹é‡å…¥å£ã€‚
* å…¶ä½™æ¥å£ä¿æŒå‘åå…¼å®¹ï¼›æ—§åå­—ä»å¯ç”¨ï¼ˆ`ClassifyPipeline = TaskPipeline`ï¼Œè§åº•éƒ¨ï¼‰ã€‚

ç”¨æ³•ç¤ºä¾‹
--------
```python
from data_pipeline_refactor import batch_process, ModelDispatcher

def my_new_task(rec, dsp: ModelDispatcher):
    # ... åœ¨è¿™é‡Œå†™ä½ çš„é€»è¾‘ ...
    return rec  # è¿”å›å¢å¼º/ä¿®æ”¹åçš„ dict

batch_process(
    input_folder="./input",
    output_folder="./output",
    api_conns=[{"api_key": "None", "base_url": "https://example.com/v1"}],
    process_fn=my_new_task,
    max_workers=64,
)
```
"""
from __future__ import annotations
import time
import itertools
import json
import logging
import os
import random
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import Any, Callable, Dict, Iterable, List, Sequence

# ----------------------------- æ—¥å¿—é…ç½® --------------------------------------
LOG_FMT = "[%(asctime)s] %(levelname)s: %(message)s"
logging.basicConfig(format=LOG_FMT, datefmt="%Y-%m-%d %H:%M:%S", level=logging.INFO)
logger = logging.getLogger(__name__)

# --------------------------- é€šç”¨é‡è¯•è£…é¥°å™¨ ----------------------------------
from functools import wraps
from time import sleep

def retry(times: int = 3, backoff: float = 1.5):
    """å¯é…ç½®çš„æŒ‡æ•°é€€é¿é‡è¯•è£…é¥°å™¨ã€‚"""

    def deco(fn):
        @wraps(fn)
        def wrapper(*args, **kwargs):
            delay = 1.0
            for i in range(times):
                try:
                    return fn(*args, **kwargs)
                except Exception as e:  # noqa: BLE001 â€“ æ•è·æ‰€æœ‰å¼‚å¸¸è®°å½•
                    if i == times - 1:
                        raise
                    logger.warning("%s å¤±è´¥ (%s)ï¼Œ%.1fs åé‡è¯•â€¦", fn.__name__, e, delay)
                    sleep(delay)
                    delay *= backoff
        return wrapper

    return deco

# --------------------------- æ¨¡å‹å®¢æˆ·ç«¯åˆ†å‘å™¨ ---------------------------------
class ModelDispatcher:
    def __init__(self, api_conns: Sequence[Dict[str, str]]):
        from openai import OpenAI
        self._OpenAI = OpenAI
        self._api_conns = list(api_conns)
        self._local = threading.local()
        self._lock = threading.Lock()
        self._round_robin = itertools.cycle(self._api_conns)

    def _pick(self):
        with self._lock:
            return next(self._round_robin)

    def _get_client(self):
        if not hasattr(self._local, "client"):
            cfg = self._pick()
            self._local.client = self._OpenAI(api_key=cfg["api_key"], base_url=cfg["base_url"])
        return self._local.client

    @retry(times=4)
    def chat(self, *args, **kwargs):
        return self._get_client().chat.completions.create(*args, **kwargs)


# -------------------------- çº¿ç¨‹æ± æ‰§è¡Œå™¨ -------------------------------------
class ThreadedExecutor:
    """æç®€çº¿ç¨‹æ± ï¼šå¹¶å‘æ‰§è¡Œä»»åŠ¡å¹¶ yield ç»“æœã€‚"""

    def __init__(self, max_workers: int = 32):
        self.max_workers = max_workers

    def run(self, fn: Callable[[Any], Any], items: Iterable[Any]):
        with ThreadPoolExecutor(self.max_workers) as pool:
            futures = {pool.submit(fn, it): it for it in items}
            for fut in as_completed(futures):
                yield fut.result()
class SmartAPIDispatcher:
    def __init__(self, api_conns: Sequence[Dict[str, str]]):
        from openai import OpenAI
        self._OpenAI = OpenAI
        self.api_conns = list(api_conns)
        
        # çŠ¶æ€è·Ÿè¸ª
        self.usage_count = [0] * len(api_conns)
        self.error_count = [0] * len(api_conns)
        self.last_used = [0] * len(api_conns)  # æ—¶é—´æˆ³
        self.lock = threading.Lock()
        
        # å®¢æˆ·ç«¯ç¼“å­˜
        self.clients = [self._create_client(i) for i in range(len(api_conns))]
    
    def _create_client(self, index):
        cfg = self.api_conns[index]
        return self._OpenAI(api_key=cfg["api_key"], base_url=cfg["base_url"])
    
    def get_client(self):
        """æ™ºèƒ½é€‰æ‹©æœ€ä¼˜APIå®¢æˆ·ç«¯"""
        with self.lock:
            # è®¡ç®—å¯ç”¨æ€§åˆ†æ•°ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
            scores = []
            current_time = time.time()
            
            for i in range(len(self.api_conns)):
                # åŸºç¡€åˆ† = 100 - é”™è¯¯è®¡æ•°*10
                score = 100 - self.error_count[i] * 10
                
                # å¢åŠ ç©ºé—²å¥–åŠ±ï¼ˆ30åˆ†é’Ÿå†…æœªä½¿ç”¨+50åˆ†ï¼‰
                if current_time - self.last_used[i] > 30:
                    score += 50
                
                # å¢åŠ ä½ä½¿ç”¨ç‡å¥–åŠ±
                usage_ratio = self.usage_count[i] / (sum(self.usage_count) or 1)
                score += int((1 - usage_ratio) * 30)
                
                scores.append(score)
            
            # é€‰æ‹©æœ€é«˜åˆ†çš„API
            selected = scores.index(max(scores))
            self.usage_count[selected] += 1
            self.last_used[selected] = current_time
            
            return self.clients[selected], selected
    
    @retry(times=3)
    def chat(self, *args, **kwargs):
        client, idx = self.get_client()
        try:
            start = time.time()
            response = client.chat.completions.create(*args, **kwargs)
            latency = time.time() - start
            
            # æˆåŠŸæ—¶å‡å°‘é”™è¯¯è®¡æ•°
            with self.lock:
                if self.error_count[idx] > 0:
                    self.error_count[idx] -= 1
                    
            logger.debug(f"API {idx} æˆåŠŸ | å»¶è¿Ÿ: {latency:.2f}s")
            return response
        
        except Exception as e:
            with self.lock:
                self.error_count[idx] += 1
            logger.warning(f"API {idx} é”™è¯¯: {str(e)}")
            raise
# --------------------------- JSONL æ–­ç‚¹å†™å…¥å™¨ -------------------------------
class JsonlCheckpointer:
    """çº¿ç¨‹å®‰å…¨æŒä¹…åŒ– + checkpointã€‚"""

    def __init__(self, output_path: os.PathLike, id_field: str = "id", state_file: os.PathLike | None = None, record_dir:str="./record"):
        self.output_path = Path(output_path)
        self.id_field = id_field

        if state_file is None:
            record_dir = record_dir/"record"
            record_dir.mkdir(parents=True, exist_ok=True)  # ç¡®ä¿æ–‡ä»¶å¤¹å­˜åœ¨
            self.state_file = record_dir / f"{self.output_path.stem}_done.txt"
        else:
            self.state_file = Path(state_file)

        self._lock = threading.Lock()
        self.processed: set[str] = self._load_state()

    def _load_state(self) -> set[str]:
        if not self.state_file.exists():
            return set()
        with self.state_file.open("r", encoding="utf-8") as fh:
            return {line.strip() for line in fh if line.strip()}

    def _persist_state(self, _id: str):
        with self.state_file.open("a", encoding="utf-8") as fh:
            fh.write(f"{_id}\n")

    # -------------------------- API ------------------------------------
    def already_done(self, _id: str | None) -> bool:
        return _id in self.processed if _id else True

    def write(self, record: Dict[str, Any]):
        _id = record.get(self.id_field)
        with self._lock:
            if _id and _id not in self.processed:
                with self.output_path.open("a", encoding="utf-8") as fh:
                    fh.write(json.dumps(record, ensure_ascii=False) + "\n")
                self.processed.add(_id)
                self._persist_state(_id)

# ----------------------------- ä¸»æµæ°´çº¿ --------------------------------------
class TaskPipeline:
    """å¹¶å‘è°ƒåº¦ + æ–­ç‚¹ç»­è·‘çš„é€šç”¨æµæ°´çº¿ã€‚"""

    Record = Dict[str, Any]
    ProcessFn = Callable[[Record, ModelDispatcher], Record]

    def __init__(self, dispatcher: ModelDispatcher, writer: JsonlCheckpointer, process_fn: ProcessFn, concurrency: int = 32):
        self.dispatcher = dispatcher
        self.writer = writer
        self.process_fn = process_fn
        self.exec = ThreadedExecutor(concurrency)

    def _process_one(self, rec: Record):
        if self.writer.already_done(rec.get("id")):
            logger.debug("è·³è¿‡ %s", rec.get("id"))
            return
        try:
            enriched = self.process_fn(rec, self.dispatcher)
            self.writer.write(enriched)
            logger.info("âœ… å®Œæˆ %s", rec.get("id"))
        except Exception as e:  # noqa: BLE001
            logger.exception("âŒ å¤„ç† %s å¤±è´¥: %s", rec.get("id"), e)

    def run_on_jsonl(self, input_path: os.PathLike):
        logger.info("ğŸš€ å¼€å§‹å¤„ç† %s", input_path)
        with open(input_path, "r", encoding="utf-8") as fh:
            records = [json.loads(line) for line in fh if line.strip()]
        print(len(records))
        for _ in self.exec.run(self._process_one, records):
            pass

# --------------------------- æ‰¹é‡æ–‡ä»¶åŠ©æ‰‹ ------------------------------------

def batch_process(
    input_folder: os.PathLike,
    output_folder: os.PathLike,
    api_conns: Sequence[Dict[str, str]],
    process_fn: TaskPipeline.ProcessFn,
    *,
    max_workers: int = 32,
):
    """å¯¹æ–‡ä»¶å¤¹å†…æ‰€æœ‰ `*.jsonl` æ‰§è¡Œ *process_fn*ï¼Œå¹¶å¤ç”¨å¹¶å‘/æ–­ç‚¹é€»è¾‘ã€‚"""
    Path(output_folder).mkdir(parents=True, exist_ok=True)
    dispatcher = ModelDispatcher(api_conns)

    for file in Path(input_folder).glob("*.jsonl"):
        out_path = Path(output_folder) / f"{file.stem}_QA.jsonl"
        writer = JsonlCheckpointer(out_path)
        pipe = TaskPipeline(dispatcher, writer, process_fn, concurrency=max_workers)
        pipe.run_on_jsonl(file)

    logger.info("ğŸ‰ æ‰¹å¤„ç†å®Œæˆï¼Œç»“æœä½äº %s", output_folder)

# ---------------------------- ç¤ºä¾‹å¤„ç†å‡½æ•° -----------------------------------

def demo_process(rec: Dict[str, Any], dsp: ModelDispatcher) -> Dict[str, Any]:
    """ç¤ºä¾‹ï¼šæŠŠ text é•¿åº¦å†™å›è®°å½•ã€‚å®é™…ä½¿ç”¨æ—¶æ›¿æ¢æ­¤å‡½æ•°ã€‚"""
    rec["text_len"] = len(rec.get("text", ""))
    return rec

# ---------------------------- ä¿æŒå‘åå…¼å®¹ ----------------------------------
ClassifyPipeline = TaskPipeline  # è€åå­—åˆ«å
batch_classify = batch_process  # è€åå­—åˆ«å

def demo_classifier(rec, dsp):  # æ—§ç¤ºä¾‹å‡½æ•°åˆ«å
    return demo_process(rec, dsp)

# ---------------------------- CLI å…¥å£ --------------------------------------
if __name__ == "__main__":
    import argparse, sys

    p = argparse.ArgumentParser(description="æ‰¹é‡å¤„ç† JSONL æ–‡ä»¶ â€“ é€šç”¨å¹¶å‘æ¡†æ¶")
    p.add_argument("input", help="åŒ…å« .jsonl çš„æ–‡ä»¶å¤¹")
    p.add_argument("output", help="è¾“å‡ºæ–‡ä»¶å¤¹")
    p.add_argument("--workers", type=int, default=32, help="å¹¶å‘çº¿ç¨‹æ•°é‡")
    args = p.parse_args()

    ENDPOINTS: List[Dict[str, str]] = [
        {"api_key": "None", "base_url": "https://example.com/v1"},
    ]

    if not ENDPOINTS:
        sys.exit("âŒ è¯·é…ç½® ENDPOINTS")

    batch_process(
        args.input,
        args.output,
        ENDPOINTS,
        process_fn=demo_process,
        max_workers=args.workers,
    )
